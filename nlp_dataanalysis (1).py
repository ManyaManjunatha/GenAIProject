# -*- coding: utf-8 -*-
"""nlp_dataanalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMbu48SiJUDdzp8OF1lSZiIbHZ4tVuXG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import warnings
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

warnings.filterwarnings('ignore')

# Downloading required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)


# Checking if required NLTK data is available
required_nltk_data = ['tokenizers/punkt', 'corpora/stopwords', 'tokenizers/punkt_tab']
all_found = True
for resource in required_nltk_data:
    try:
        nltk.data.find(resource)
        print(f"✓ NLTK resource '{resource}' found")
    except LookupError:
        print(f"✗ NLTK resource '{resource}' not found")
        all_found = False

if all_found:
    print("✓ All required NLTK dependencies loaded successfully")
else:
    print("✗ Some required NLTK dependencies were not found.")

"""## Configuration"""

# Configuration
STOP_WORDS = set(stopwords.words('english'))
MIN_WORD_LENGTH = 3

print(f"Stop words loaded: {len(STOP_WORDS)} words")
print(f"Minimum word length: {MIN_WORD_LENGTH} characters")

"""## Utility Functions"""

def load_data(filepath):
    try:
        df = pd.read_csv(filepath)
        print(f"✓ Data Loaded Successfully – {df.shape[0]} posts, {df.shape[1]} features\n")
        return df
    except FileNotFoundError:
        print(f"✗ Error: File '{filepath}' not found")
        return None
    except Exception as e:
        print(f"✗ Error loading data: {e}")
        return None

def validate_columns(df, required_cols):
    """Check if required columns exist"""
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        print(f"✗ Missing columns: {missing}")
        return False
    return True

def clean_text(text):
    """clean text"""
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)          # Remove URLs
    text = re.sub(r"[^a-z\s]", " ", text)        # Remove special characters
    text = ' '.join(text.split())                # Remove extra spaces
    return text

def extract_words(text):
    """Tokenize and filter words """
    tokens = word_tokenize(text)
    return [w for w in tokens if len(w) >= MIN_WORD_LENGTH and w not in STOP_WORDS]

print("✓ Utility functions defined")

"""## Loading Dataset"""

print("=" * 80)
print("LOADING DATASET")
print("=" * 80)

df = load_data("generative_ai_misinformation_dataset.csv")

if df is not None:
    print(df.head())
else:
    print("Failed to load data. Please check the file path.")

"""## Validating Columns"""

# Validate required columns
required_cols = ['text', 'is_misinformation']

if not validate_columns(df, required_cols):
    print("Missing required columns. Stopping execution.")
else:
    print("✓ All required columns present")

"""## Data Cleaning"""

print("=" * 80)
print("DATA CLEANING")
print("=" * 80)

missing_before = df.isnull().sum().sum()
print(f"Missing values found: {missing_before}")

df = df.dropna()
df = df.reset_index(drop=True)
print(f"✓ Remaining posts after cleaning: {len(df)}\n")

# Text cleaning
df["cleaned_text"] = df["text"].apply(clean_text)
print("Example Text Cleaning:")
print(f"Before: {df['text'].iloc[0][:80]}...")
print(f"After:  {df['cleaned_text'].iloc[0][:80]}...")

"""## Calculate Statistics"""

print("\n" + "=" * 80)
print("CALCULATING STATISTICS")
print("=" * 80)

total_posts = len(df)
misinformation_mask = df["is_misinformation"] == 1
fake_count = misinformation_mask.sum()
real_count = (~misinformation_mask).sum()
fake_percentage = (fake_count / total_posts) * 100
real_percentage = (real_count / total_posts) * 100
avg_text_length = df["text"].str.len().mean()

print(f"Total posts: {total_posts:,}")
print(f"Real posts: {real_count:,} ({real_percentage:.1f}%)")
print(f"Fake posts: {fake_count:,} ({fake_percentage:.1f}%)")
print(f"Average text length: {avg_text_length:.1f} characters\n")

"""## Real vs Fake Posts Visualization"""

fig, ax = plt.subplots(figsize=(7, 6))
ax.pie([real_count, fake_count],
       labels=["Real Posts", "Fake News"],
       autopct="%1.1f%%", colors=["lightblue", "salmon"], startangle=90)
ax.set_title("Real vs Fake Posts", fontsize=16, weight="bold")
plt.tight_layout()
plt.show()

"""## Platform-Wise Analysis"""

platform_stats = None

if "platform" in df.columns:
    platform_stats = df.groupby("platform")["is_misinformation"].mean() * 100
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.barplot(x=platform_stats.index, y=platform_stats.values, palette="flare", ax=ax)
    ax.set_title("Fake News Rate by Platform (%)", fontsize=16, weight="bold")
    ax.set_ylabel("Fake News Rate (%)")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    print("✓ Platform analysis completed\n")
else:
    print("⚠ No 'platform' column found, skipping platform analysis\n")

"""## Feature Correlation Matrix"""

numeric_df = df.select_dtypes(include=['number'])

if not numeric_df.empty:
    fig, ax = plt.subplots(figsize=(8, 6))
    corr = numeric_df.corr()
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", ax=ax)
    ax.set_title("Feature Correlation Matrix", fontsize=14, weight="bold")
    plt.tight_layout()
    plt.show()
else:
    print("⚠ No numeric columns available for correlation analysis\n")

"""## Common Word Analysis in Fake News"""

print("=" * 80)
print("COMMON WORD ANALYSIS IN FAKE NEWS")
print("=" * 80)

fake_posts = df[misinformation_mask]["cleaned_text"]
top_words = []

if len(fake_posts) > 0:
    # Extract and count words efficiently
    all_words = []
    for text in fake_posts:
        words = extract_words(text)
        all_words.extend(words)

    word_counts = Counter(all_words)
    top_words = word_counts.most_common(15)

    if top_words:
        words_list, counts_list = zip(*top_words)
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x=list(counts_list), y=list(words_list), color="red", ax=ax)
        ax.set_title("Top 15 Words in Fake News", fontsize=16, weight="bold")
        ax.set_xlabel("Frequency")
        ax.set_ylabel("Word")
        plt.tight_layout()
        plt.show()
        print(f"✓ Word analysis completed\n")
    else:
        print("⚠ No words found for analysis\n")
else:
    print("⚠ No fake posts to analyze\n")

"""## Final Summary Report"""

print("=" * 80)
print("FINAL SUMMARY REPORT")
print("=" * 80)

most_common_word = top_words[0][0] if top_words else "N/A"
common_word_count = top_words[0][1] if top_words else 0
other_words = ', '.join([w for w, _ in top_words[1:5]]) if len(top_words) > 1 else "N/A"

top_platform = platform_stats.idxmax() if platform_stats is not None else "N/A"
top_platform_rate = platform_stats.max() if platform_stats is not None else 0

summary = f"""
{'=' * 60}
DATA OVERVIEW
{'=' * 60}
Total posts analyzed: {total_posts:,}
Real posts: {real_count:,} ({real_percentage:.1f}%)
Fake posts: {fake_count:,} ({fake_percentage:.1f}%)
Average text length: {avg_text_length:.1f} characters per post

{'=' * 60}
PLATFORM INSIGHTS
{'=' * 60}
Most misinformation found on: {top_platform}
Fake news rate on that platform: {top_platform_rate:.2f}%

{'=' * 60}
TEXTUAL INSIGHTS
{'=' * 60}
Most common fake-news keyword: '{most_common_word}' (appears {common_word_count} times)
Other frequently used words: {other_words}

{'=' * 60}
KEY FINDINGS
{'=' * 60}
• The dataset contains {fake_percentage:.1f}% fake news posts
• Fake news tends to use repeated keywords like '{most_common_word}'
• {('Platform data was analyzed; highest rate on ' + top_platform) if platform_stats is not None else 'Platform data not available'}
• Text cleaning successfully standardized all posts for NLP processing
"""

print(summary)